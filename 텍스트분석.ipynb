{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9969b9",
   "metadata": {},
   "source": [
    "# nltk 다운받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2e5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ad\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.60.0)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.4.4-cp38-cp38-win_amd64.whl (270 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\ad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Installing collected packages: click, regex, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.6.2 regex-2021.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a839c6",
   "metadata": {},
   "source": [
    "## 잘다운받아졌나확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab425df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk, version 3.6.2\n"
     ]
    }
   ],
   "source": [
    "!nltk --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8a35f",
   "metadata": {},
   "source": [
    "## 문장단위로 잘라보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41ee16b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.']\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import sent_tokenize\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. You can see it out your window or on your television.'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896bf77",
   "metadata": {},
   "source": [
    "## 단어단위로 잘라보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f278883e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 45\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.', 'You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.', 'you', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']\n"
     ]
    }
   ],
   "source": [
    "#단어 토큰화  : word_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentences = 'The Matrix is everywhere its all around us, here even in this room. You can see it out your window or on your television. you feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "words = word_tokenize(sentences)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d8638",
   "metadata": {},
   "source": [
    "## 여러 문장들에 대한 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bce07853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 문장들에 대한 단어 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentences) for sentences in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "#여러 문장들에 대해 문장별 단어 토큰화 수행.\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8447672",
   "metadata": {},
   "source": [
    "## stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "895b8239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134cca0a",
   "metadata": {},
   "source": [
    "### 영어 stopwords 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367b9e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 179\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71bc26",
   "metadata": {},
   "source": [
    "### 영어 stopwords  출력해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cfcf2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words('english')[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d4cfd",
   "metadata": {},
   "source": [
    "### 위 예제의 3개의 문장에서 얻은 단어 토큰에 대하여 stopwords 제거해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ecdfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 위의 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 tokenize된 sentence List에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환하쟈\n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word 가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d083de5",
   "metadata": {},
   "source": [
    "## stemming 과 lemmationzation 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f12804",
   "metadata": {},
   "source": [
    "### stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0f7597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amuz\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amuzed'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8685f3c",
   "metadata": {},
   "source": [
    "### 3Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f886709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec20a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
